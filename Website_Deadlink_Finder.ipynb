{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOxq1g0UB/j8CLT4KYQ6Tp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrKenReid/DeadLinkScraper/blob/main/Website_Deadlink_Finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Website Deadlink Scraper\n",
        "\n",
        "Written by [Ken Reid](https://github.com/DrKenReid).\n",
        "\n",
        "This notebook contains a Python script for scanning websites and identifying dead links.\n",
        "\n",
        "## How to Use:\n",
        "\n",
        "1. Run the cell below.\n",
        "2. When prompted, enter the base URL of the website you want to scan (e.g., 'example.com').\n",
        "3. The script will mount your Google Drive and create a folder structure for result - so accept the connection to your Google Drive.\n",
        "4. Scanning progress will be displayed in real-time.\n",
        "5. Results are saved in your Google Drive under 'WebScraperResults/[website_name]'.\n",
        "\n",
        "## Features:\n",
        "\n",
        "- Multithreaded scanning for improved performance\n",
        "- Saves scan history to avoid unnecessary rescans\n",
        "- Handles subdomains and different URL formats\n",
        "- Detailed logging and error handling\n",
        "\n",
        "Note: Please use responsibly and respect the website's robots.txt file and terms of service."
      ],
      "metadata": {
        "id": "8NPTQ-Ot6efD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnwxIkka2eTu"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "import time\n",
        "import traceback\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "class WebsiteDeadlinkScraper:\n",
        "    def __init__(self, base_url, drive_folder='WebScraperResults'):\n",
        "        self.setup_logging()\n",
        "        self.logger.info(\"Initializing WebsiteDeadlinkScraper\")\n",
        "        self.drive_folder = drive_folder\n",
        "        self.results_file = 'deadlinks.csv'\n",
        "        self.history_file = 'scan_history.csv'\n",
        "        self.max_pages = 10000\n",
        "        self.max_depth = 20\n",
        "        self.visited_urls = set()\n",
        "        self.deadlinks = []\n",
        "        self.current_depth = 0\n",
        "        self.base_url = self.format_and_verify_url(base_url)\n",
        "        self.website_folder = urlparse(self.base_url).netloc\n",
        "        self.setup()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def format_and_verify_url(self, url):\n",
        "        if not url.startswith(('http://', 'https://')):\n",
        "            url = 'http://' + url\n",
        "\n",
        "        parsed_url = urlparse(url)\n",
        "        if not parsed_url.netloc.startswith('www.'):\n",
        "            url = parsed_url._replace(netloc='www.' + parsed_url.netloc).geturl()\n",
        "\n",
        "        self.logger.info(f\"Attempting to connect to {url}\")\n",
        "        if self.check_url_accessibility(url):\n",
        "            return url\n",
        "\n",
        "        # If http fails, try https\n",
        "        if url.startswith('http://'):\n",
        "            https_url = 'https://' + url[7:]\n",
        "            self.logger.info(f\"HTTP failed. Attempting to connect to {https_url}\")\n",
        "            if self.check_url_accessibility(https_url):\n",
        "                return https_url\n",
        "\n",
        "        self.logger.error(\"Failed to connect to the website. Please check the URL and try again.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    def check_url_accessibility(self, url):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            return response.status_code == 200\n",
        "        except requests.RequestException:\n",
        "            return False\n",
        "\n",
        "    def setup(self):\n",
        "        self.mount_drive()\n",
        "        self.create_folder_and_files()\n",
        "        self.load_history()\n",
        "        self.load_existing_results()\n",
        "\n",
        "    def mount_drive(self):\n",
        "        try:\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error mounting Google Drive: {str(e)}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        self.drive_path = f'/content/drive/My Drive/{self.drive_folder}/{self.website_folder}/'\n",
        "        self.logger.info(f\"Drive mounted successfully. Working directory: {self.drive_path}\")\n",
        "\n",
        "    def create_folder_and_files(self):\n",
        "        os.makedirs(self.drive_path, exist_ok=True)\n",
        "\n",
        "        results_path = os.path.join(self.drive_path, self.results_file)\n",
        "        if not os.path.exists(results_path):\n",
        "            pd.DataFrame(columns=['source', 'deadlink']).to_csv(results_path, index=False)\n",
        "            self.logger.info(f\"Created results file: {results_path}\")\n",
        "\n",
        "        history_path = os.path.join(self.drive_path, self.history_file)\n",
        "        if not os.path.exists(history_path):\n",
        "            pd.DataFrame(columns=['URL', 'LastScanned']).to_csv(history_path, index=False)\n",
        "            self.logger.info(f\"Created history file: {history_path}\")\n",
        "\n",
        "    def load_history(self):\n",
        "        history_path = os.path.join(self.drive_path, self.history_file)\n",
        "        self.history_df = pd.read_csv(history_path)\n",
        "        self.history_df['LastScanned'] = pd.to_datetime(self.history_df['LastScanned'])\n",
        "\n",
        "    def load_existing_results(self):\n",
        "        results_path = os.path.join(self.drive_path, self.results_file)\n",
        "        self.deadlinks = pd.read_csv(results_path).to_dict('records')\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        parsed_url = urlparse(url)\n",
        "        base_parsed = urlparse(self.base_url)\n",
        "        return parsed_url.netloc == base_parsed.netloc or parsed_url.netloc.endswith(base_parsed.netloc)\n",
        "\n",
        "    def check_link(self, url):\n",
        "        try:\n",
        "            response = requests.head(url, allow_redirects=True, timeout=5)\n",
        "            return response.status_code != 200\n",
        "        except requests.RequestException:\n",
        "            return True\n",
        "\n",
        "    def scrape_page(self, url, depth, force_scan=False):\n",
        "        if url in self.visited_urls or depth > self.max_depth:\n",
        "            return []\n",
        "\n",
        "        self.visited_urls.add(url)\n",
        "        self.current_depth = max(self.current_depth, depth)\n",
        "\n",
        "        # Check if the page was scanned in the last 14 days\n",
        "        if not force_scan and url in self.history_df['URL'].values:\n",
        "            last_scanned = self.history_df.loc[self.history_df['URL'] == url, 'LastScanned'].iloc[0]\n",
        "            if datetime.now() - last_scanned < timedelta(days=14):\n",
        "                return []\n",
        "\n",
        "        self.update_progress(f\"Scanning: {url} (Depth: {depth})\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            internal_links = []\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                full_url = urljoin(url, link['href'])\n",
        "                if self.is_valid_url(full_url):\n",
        "                    if self.check_link(full_url):\n",
        "                        self.deadlinks.append({'source': url, 'deadlink': full_url})\n",
        "                        self.save_result({'source': url, 'deadlink': full_url})\n",
        "                    elif full_url not in self.visited_urls:\n",
        "                        internal_links.append((full_url, depth + 1))\n",
        "\n",
        "            # Update history\n",
        "            self.update_history(url)\n",
        "            return internal_links\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error scanning {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def start_scraping(self):\n",
        "        to_visit = [(self.base_url, 0)]  # (url, depth)\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            while to_visit and len(self.visited_urls) < self.max_pages:\n",
        "                futures = []\n",
        "                batch = to_visit[:10]  # Take up to 10 URLs to process in parallel\n",
        "                to_visit = to_visit[10:]  # Remove the processed batch from the queue\n",
        "\n",
        "                for url, depth in batch:\n",
        "                    force_scan = (url == self.base_url)  # Force scan for the initial URL\n",
        "                    futures.append(executor.submit(self.scrape_page, url, depth, force_scan))\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    new_links = future.result()\n",
        "                    to_visit.extend(new_links)\n",
        "\n",
        "                self.update_progress(f\"Queue size: {len(to_visit)}\")\n",
        "\n",
        "        self.update_progress(\"Scraping completed.\")\n",
        "        self.logger.info(f\"Scanned {len(self.visited_urls)} pages, found {len(self.deadlinks)} dead links, reached depth {self.current_depth}\")\n",
        "\n",
        "    def save_result(self, result):\n",
        "        df = pd.DataFrame([result])\n",
        "        results_path = os.path.join(self.drive_path, self.results_file)\n",
        "        df.to_csv(results_path, mode='a', header=False, index=False)\n",
        "\n",
        "    def update_history(self, url):\n",
        "        if url in self.history_df['URL'].values:\n",
        "            self.history_df.loc[self.history_df['URL'] == url, 'LastScanned'] = datetime.now()\n",
        "        else:\n",
        "            new_row = pd.DataFrame({'URL': [url], 'LastScanned': [datetime.now()]})\n",
        "            self.history_df = pd.concat([self.history_df, new_row], ignore_index=True)\n",
        "\n",
        "        self.save_history()\n",
        "\n",
        "    def save_history(self):\n",
        "        self.history_df.to_csv(os.path.join(self.drive_path, self.history_file), index=False)\n",
        "\n",
        "    def update_progress(self, message):\n",
        "        progress = f\"\\rScanned: {len(self.visited_urls)} pages, Found: {len(self.deadlinks)} deadlinks, Max Depth: {self.current_depth}, {message}\"\n",
        "        sys.stdout.write(progress)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        base_url = input(\"Enter the base URL to scrape (e.g., example.com): \")\n",
        "        scraper = WebsiteDeadlinkScraper(base_url)\n",
        "        scraper.start_scraping()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred: {str(e)}\")\n",
        "        logging.error(\"Traceback:\")\n",
        "        traceback.print_exc()\n",
        "        logging.error(\"Please check the error message and ensure Google Drive is properly mounted.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}