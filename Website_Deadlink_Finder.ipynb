{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpU96k0bVvHk7BywOeDlqD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrKenReid/DeadLinkScraper/blob/main/Website_Deadlink_Finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnwxIkka2eTu",
        "outputId": "b0182109-5ae2-45e5-effc-2015946267dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the base URL to scrape (e.g., https://example.com): https://midas.umich.edu/\n",
            "Mounted at /content/drive\n",
            "Drive mounted successfully. Working directory: /content/drive/My Drive/WebScraperResults/\n",
            "Scanned: 2 pages, Found: 0 deadlinks, Max Depth: 1, Scanning: https://midas.umich.edu/#ajax-content-wrap (Depth: 1)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d44c38642ede>:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.history_df = pd.concat([self.history_df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanned: 4 pages, Found: 0 deadlinks, Max Depth: 1, Scanning: https://midas.umich.edu/#sidewidgetarea (Depth: 1)"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "class WebsiteDeadlinkScraper:\n",
        "    def __init__(self, base_url, drive_folder='WebScraperResults'):\n",
        "        \"\"\"Initialize the website deadlink scraper.\"\"\"\n",
        "        self.base_url = base_url\n",
        "        self.drive_folder = drive_folder\n",
        "        self.results_file = 'deadlinks.csv'\n",
        "        self.history_file = 'scan_history.csv'\n",
        "        self.max_pages = 10000\n",
        "        self.max_depth = 20\n",
        "        self.visited_urls = set()\n",
        "        self.deadlinks = []\n",
        "        self.current_depth = 0\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"Set up the scraper by mounting drive and loading necessary data.\"\"\"\n",
        "        self.mount_drive()\n",
        "        self.load_history()\n",
        "        self.load_existing_results()\n",
        "\n",
        "    def mount_drive(self):\n",
        "        \"\"\"Mount Google Drive and set up the drive path.\"\"\"\n",
        "        try:\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error mounting Google Drive: {str(e)}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        self.drive_path = f'/content/drive/My Drive/{self.drive_folder}/'\n",
        "        os.makedirs(self.drive_path, exist_ok=True)\n",
        "        print(f\"Drive mounted successfully. Working directory: {self.drive_path}\")\n",
        "\n",
        "    def load_history(self):\n",
        "        \"\"\"Load scan history from CSV file or create a new DataFrame if it doesn't exist.\"\"\"\n",
        "        history_path = os.path.join(self.drive_path, self.history_file)\n",
        "        if os.path.exists(history_path):\n",
        "            self.history_df = pd.read_csv(history_path)\n",
        "            self.history_df['LastScanned'] = pd.to_datetime(self.history_df['LastScanned'])\n",
        "        else:\n",
        "            self.history_df = pd.DataFrame(columns=['URL', 'LastScanned'])\n",
        "\n",
        "    def load_existing_results(self):\n",
        "        \"\"\"Load existing results from CSV file if it exists.\"\"\"\n",
        "        results_path = os.path.join(self.drive_path, self.results_file)\n",
        "        if os.path.exists(results_path):\n",
        "            self.deadlinks = pd.read_csv(results_path).to_dict('records')\n",
        "        else:\n",
        "            self.deadlinks = []\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"Check if a URL is valid and belongs to the same domain as the base URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc == urlparse(self.base_url).netloc\n",
        "\n",
        "    def check_link(self, url):\n",
        "        \"\"\"Check if a link is dead (returns a non-200 status code).\"\"\"\n",
        "        try:\n",
        "            response = requests.head(url, allow_redirects=True, timeout=5)\n",
        "            return response.status_code != 200\n",
        "        except requests.RequestException:\n",
        "            return True\n",
        "\n",
        "    def scrape_page(self, url, depth, force_scan=False):\n",
        "        \"\"\"Scrape a single page for deadlinks and collect internal links for further scraping.\"\"\"\n",
        "\n",
        "        if url in self.visited_urls or depth > self.max_depth:\n",
        "            return []\n",
        "\n",
        "        self.visited_urls.add(url)\n",
        "        self.current_depth = max(self.current_depth, depth)\n",
        "\n",
        "        # Check if the page was scanned in the last 14 days\n",
        "        if not force_scan and url in self.history_df['URL'].values:\n",
        "            last_scanned = self.history_df.loc[self.history_df['URL'] == url, 'LastScanned'].iloc[0]\n",
        "            if datetime.now() - last_scanned < timedelta(days=14):\n",
        "                return []\n",
        "\n",
        "        self.update_progress(f\"Scanning: {url} (Depth: {depth})\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            internal_links = []\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                full_url = urljoin(url, link['href'])\n",
        "                if self.is_valid_url(full_url):\n",
        "                    if self.check_link(full_url):\n",
        "                        self.deadlinks.append({'source': url, 'deadlink': full_url})\n",
        "                        self.save_result({'source': url, 'deadlink': full_url})\n",
        "                    elif full_url not in self.visited_urls:\n",
        "                        internal_links.append((full_url, depth + 1))\n",
        "\n",
        "            # Update history\n",
        "            self.update_history(url)\n",
        "            return internal_links\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Error scanning {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def start_scraping(self):\n",
        "        \"\"\"Start the scraping process from the base URL.\"\"\"\n",
        "        to_visit = [(self.base_url, 0)]  # (url, depth)\n",
        "        while to_visit and len(self.visited_urls) < self.max_pages:\n",
        "            url, depth = to_visit.pop(0)\n",
        "            force_scan = (url == self.base_url)  # Force scan for the initial URL\n",
        "            new_links = self.scrape_page(url, depth, force_scan)\n",
        "            to_visit.extend(new_links)\n",
        "            self.update_progress(f\"Queue size: {len(to_visit)}\")\n",
        "\n",
        "        self.update_progress(\"Scraping completed.\")\n",
        "\n",
        "    def save_result(self, result):\n",
        "        \"\"\"Save a single result to the CSV file.\"\"\"\n",
        "        df = pd.DataFrame([result])\n",
        "        results_path = os.path.join(self.drive_path, self.results_file)\n",
        "        df.to_csv(results_path, mode='a', header=not os.path.exists(results_path), index=False)\n",
        "\n",
        "    def update_history(self, url):\n",
        "        \"\"\"Update the scan history for a single URL and save it.\"\"\"\n",
        "        if url in self.history_df['URL'].values:\n",
        "            self.history_df.loc[self.history_df['URL'] == url, 'LastScanned'] = datetime.now()\n",
        "        else:\n",
        "            new_row = pd.DataFrame({'URL': [url], 'LastScanned': [datetime.now()]})\n",
        "            self.history_df = pd.concat([self.history_df, new_row], ignore_index=True)\n",
        "\n",
        "        self.save_history()\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"Save the entire scan history to a CSV file in Google Drive.\"\"\"\n",
        "        self.history_df.to_csv(os.path.join(self.drive_path, self.history_file), index=False)\n",
        "\n",
        "    def update_progress(self, message):\n",
        "        \"\"\"Update the progress message on a single line.\"\"\"\n",
        "        progress = f\"\\rScanned: {len(self.visited_urls)} pages, Found: {len(self.deadlinks)} deadlinks, Max Depth: {self.current_depth}, {message}\"\n",
        "        sys.stdout.write(progress)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        base_url = input(\"Enter the base URL to scrape (e.g., https://example.com): \")\n",
        "        scraper = WebsiteDeadlinkScraper(base_url)\n",
        "        scraper.start_scraping()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred: {str(e)}\")\n",
        "        print(\"Traceback:\")\n",
        "        traceback.print_exc()\n",
        "        print(\"Please check the error message and ensure Google Drive is properly mounted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJqsKrJG2hl4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}